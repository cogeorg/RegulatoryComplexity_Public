{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420d68f5",
   "metadata": {},
   "source": [
    "# download from Github and content extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52023403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of functions\n",
    "\n",
    "# extract .txt file names\n",
    "def extract_txt_file_names(data):\n",
    "    # Extract all the txt.files names\n",
    "    txt_files = []\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key == 'items' and isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if item.get('contentType') == 'file' and item.get('name').endswith('.txt'):\n",
    "                        file_name = item.get('name')\n",
    "                        txt_files.append(file_name)\n",
    "            else:\n",
    "                txt_files.extend(extract_txt_file_names(value))\n",
    "    return txt_files\n",
    "\n",
    "\n",
    "# extract txt files content\n",
    "def extract_content(txt_file_names):\n",
    "    base_url = \"https://raw.githubusercontent.com/cogeorg/RegulatoryComplexity_Public/main/010_cleaned_data/DoddFrank/Sections/\"\n",
    "\n",
    "    contents=[]\n",
    "    for i in trange(len(txt_file_names)):\n",
    "        file_url = base_url + txt_file_names[i]\n",
    "        content= ' '.join(requests.get(file_url).content.decode('utf-8').replace('\\n', ' ').split())\n",
    "        contents.append(content)\n",
    "    return contents    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3746658",
   "metadata": {},
   "source": [
    "# sentence wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c21d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each text into sentences\n",
    "def split_into_sentences(text):\n",
    "    # Regular expression for splitting into sentences\n",
    "    sentence_splitter = re.compile(r'(?<=\\S\\.)\\s')\n",
    "    return sentence_splitter.split(text)\n",
    "\n",
    "# preprocess the text by removing punctuation, convert to lower case and apply pos tag and lemmatization\n",
    "def preprocessing(sentences):\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize and assign pos_tag\n",
    "        tokens = pos_tag(word_tokenize(sentence))\n",
    "        \n",
    "        # lemmatize and convert to lower case\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens=[(lemmatizer.lemmatize(word.lower()), tag) for word, tag in tokens]\n",
    "        \n",
    "        # Remove specific tokens\n",
    "        stop_words=['gt','lt']\n",
    "        tokens = [(word, tag) for word, tag in tokens if word not in stop_words]\n",
    "      \n",
    "        # remove punctuation\n",
    "        punctuation = set(string.punctuation).union({'--', ':', ',', \"''\", '``', 'EN.  '})\n",
    "        tokens = [(word, tag) for word, tag in tokens if word not in punctuation]\n",
    "\n",
    "        preprocessed_sentences.append(tokens)\n",
    "\n",
    "    return preprocessed_sentences\n",
    "\n",
    "# find all possible ngrams of a specific length and that appear at least a given number of times\n",
    "def get_ngrams_df(preprocessed_sentences, n, threshold):\n",
    "    # Generate ngrams for each sentence\n",
    "    all_ngrams = []\n",
    "    for sentence in preprocessed_sentences:\n",
    "        # Ensure the sentence length is at least n\n",
    "        if len(sentence) >= n:\n",
    "            sentence_ngrams = ngrams(sentence, n)\n",
    "           # all_ngrams.extend(sentence_ngrams)\n",
    "            all_ngrams.extend([ngram for ngram in sentence_ngrams if not any('*' in word for word, _ in ngram)])\n",
    "\n",
    "    # Count frequencies of ngrams\n",
    "    ngram_freq = Counter(all_ngrams)\n",
    "\n",
    "    # Filter ngrams that meet the threshold\n",
    "    frequent_ngrams = [(ngram, count) for ngram, count in ngram_freq.items() if count >= threshold]\n",
    "\n",
    "    # Sort by most frequent\n",
    "    frequent_ngrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Transform each ngram to the desired format\n",
    "    frequent_ngrams_transformed = [{'Ngram': ngram, 'Count': count, \n",
    "                                    'Words': tuple(word for word, _ in ngram), \n",
    "                                    'POSTags': tuple(tag for _, tag in ngram)} \n",
    "                                   for ngram, count in frequent_ngrams]\n",
    "\n",
    "    # Create DataFrame\n",
    "    ngrams_df = pd.DataFrame(frequent_ngrams_transformed)\n",
    "\n",
    "    # Check for duplicates and add filters\n",
    "    word_set_counts = Counter(ngram['Words'] for ngram in frequent_ngrams_transformed)\n",
    "    for ngram in ngrams_df.itertuples():\n",
    "        ngrams_df.at[ngram.Index, 'Has_duplicates'] = 1 if word_set_counts[ngram.Words] > 1 else 0\n",
    "        ngrams_df.at[ngram.Index, 'contain_digit'] = 1 if any(char.isdigit() for word in ngram.Words for char in word) else 0\n",
    "        ngrams_df.at[ngram.Index, 'contain_single_letter'] = 1 if any(len(word) == 1 and word.isalpha() for word in ngram.Words) else 0\n",
    "        ngrams_df.at[ngram.Index, 'Length'] = n\n",
    "\n",
    "    # Drop the 'Ngram' column\n",
    "    ngrams_df = ngrams_df.drop('Ngram', axis=1)\n",
    "\n",
    "    # Reorder the columns\n",
    "    ngrams_df = ngrams_df[['Words', 'POSTags', 'Length','Count', 'Has_duplicates', 'contain_digit', 'contain_single_letter']]\n",
    "\n",
    "    return ngrams_df\n",
    "\n",
    "\n",
    "# Convert ngram tuple to string\n",
    "def ngram_to_string(ngram):\n",
    "    return ' '.join(ngram)\n",
    "\n",
    "# Find indices where ngram with specific tags occurs within sentences\n",
    "def find_ngram_with_tags(sentences_words, sentences_tags, ngram_words, ngram_tags):\n",
    "    n = len(ngram_words)\n",
    "    indices = []\n",
    "    for sentence_index, (sentence_words, sentence_tags) in enumerate(zip(sentences_words, sentences_tags)):\n",
    "        for i in range(len(sentence_words) - n + 1):\n",
    "            if sentence_words[i:i + n] == ngram_words and sentence_tags[i:i + n] == ngram_tags:\n",
    "                indices.append(sentence_index)\n",
    "    return indices\n",
    "\n",
    "# Process each ngram with its tags considering the sentence structure (manipulation)\n",
    "def process_ngram_with_tags(ngram, ngram_tags, texts, tags, filenames):\n",
    "    ngram_words = ngram_to_string(ngram).split()\n",
    "    ngram_tags = ngram_to_string(ngram_tags).split()\n",
    "    all_occurrences = [(text, find_ngram_with_tags(text, tag, ngram_words, ngram_tags), filename) \n",
    "                       for text, tag, filename in zip(texts, tags, filenames)]\n",
    "\n",
    "    # Flatten occurrences and sample up to 3 unique sentences\n",
    "    all_sentences_with_ngram = [(text_sentences[index], filename) \n",
    "                                for text_sentences, indices, filename in all_occurrences \n",
    "                                for index in indices]\n",
    "    \n",
    "    random.shuffle(all_sentences_with_ngram)\n",
    "    sampled_sentences = random.sample(all_sentences_with_ngram, min(3, len(all_sentences_with_ngram)))\n",
    "\n",
    "   # occurrence_counts = sum(len(indices) for _, indices, _ in all_occurrences)\n",
    "    contexts, context_filenames = zip(*sampled_sentences) if sampled_sentences else ([], [])\n",
    "    #occurrence_counts\n",
    "    return [' '.join(sentence) for sentence in contexts], list(context_filenames)\n",
    "\n",
    "\n",
    "# remove ngrams from the text\n",
    "def remove_ngrams_from_text(ngrams_df, df, source_text='preprocessed_text'):\n",
    "    # Check if 'ngram_removal_text' column exists and adjust source_text accordingly\n",
    "    if 'ngram_removal_text' in df.columns:\n",
    "        source_text = 'ngram_removal_text'\n",
    "    \n",
    "    # Extract ngrams and convert from tokenized version into string\n",
    "    ngram_strings = [' '.join(token) for token in ngrams_df['Words'].tolist()]\n",
    "    ngram_tags = [' '.join(token) for token in ngrams_df['POSTags'].tolist()]\n",
    "\n",
    "    # Convert text from tokenized version into string while maintaining the original grouping\n",
    "    text_strings = []\n",
    "    text_tags = []\n",
    "    for i in range(len(df)):\n",
    "        # Collect all sentences and tags for the current row into separate lists\n",
    "        sentences = [' '.join(token) for token in df[source_text][i]]\n",
    "        tags = [' '.join(token) for token in df['preprocessed_tags'][i]]\n",
    "        # Append these lists to text_strings and text_tags, preserving the row-wise grouping\n",
    "        text_strings.append(sentences)\n",
    "        text_tags.append(tags)\n",
    "\n",
    "    # Iterate over each text string and its corresponding tags to substitute ngrams with '*'\n",
    "    for i in range(len(text_strings)):\n",
    "        for j, (sentence, tags) in enumerate(zip(text_strings[i], text_tags[i])):\n",
    "            modified_sentence = sentence  # Initialize modified sentence\n",
    "            for k, ngram in enumerate(ngram_strings):\n",
    "                ngram_tag = ngram_tags[k]\n",
    "                if ngram in modified_sentence and ngram_tag in tags:\n",
    "                    # Create the replacement string\n",
    "                    replacement = ' '.join(['*' for _ in ngram.split()])\n",
    "                    # Replace the ngram in the modified sentence with the replacement string\n",
    "                    modified_sentence = modified_sentence.replace(ngram, replacement)\n",
    "            # Update the sentence in text_strings with the modified sentence\n",
    "            text_strings[i][j] = modified_sentence\n",
    "\n",
    "    # Combine the processed words and tags back into the desired format\n",
    "    text_strings_tag = [[[(word, tag) for word, tag in zip(sentence.split(), tags.split())] for sentence, tags in zip(paragraph, tag_paragraph)] for paragraph, tag_paragraph in zip(text_strings, text_tags)]\n",
    "\n",
    "    # Update the DataFrame in-place\n",
    "    df['ngram_removal'] = text_strings_tag\n",
    "    df['ngram_removal_text'] = [[[word for word, _ in sentence] for sentence in row] for row in df['ngram_removal']]\n",
    "\n",
    "    return df\n",
    "\n",
    "# import validated excel file and manipulate them to prepare for next iteration\n",
    "def process_ngrams_file(filepath):\n",
    "    # Read the data\n",
    "    df = pd.read_excel(filepath)\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    df = df[df['include'] == 1]\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['include', 'is_in_dictionary'])\n",
    "    \n",
    "    # Convert string representations of lists back into actual lists\n",
    "    for column in ['Words', 'POSTags', 'Random_Examples', 'Example_Filenames']:\n",
    "        df[column] = df[column].apply(ast.literal_eval)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# apply the ngram search and removal iteratively till n=2 included.\n",
    "def get_ngrams_iteration(df, n, threshold, final_ngrams_df):\n",
    "    i = n  # Start from n\n",
    "    if i < 4:\n",
    "        threshold = 2*threshold\n",
    "    while i >= 2:  # Ensure i doesn't go below 2\n",
    "        # Flatten the list of sentences into a single list for n-gram processing\n",
    "        flattened_sentences = [word_tag for sentence in df['ngram_removal'] for word_tag in sentence]\n",
    "        try:\n",
    "            ngrams_df = get_ngrams_df(flattened_sentences, n=i, threshold=threshold)\n",
    "            \n",
    "            print(f'There are {len(ngrams_df)} n-grams of length {i}, of which {len(ngrams_df[ngrams_df[\"Has_duplicates\"] == 1])} are duplicates')\n",
    "\n",
    "            # Add the examples variable\n",
    "            ngrams_df['Random_Examples'], ngrams_df['Example_Filenames'] = zip(*ngrams_df.progress_apply(lambda row: process_ngram_with_tags(row['Words'], row['POSTags'], df['preprocessed_text'].tolist(), df['preprocessed_tags'].tolist(), df['filename'].tolist()), axis=1))\n",
    "\n",
    "            final_ngrams_df = pd.concat([final_ngrams_df, ngrams_df])\n",
    "\n",
    "            # Remove ngrams\n",
    "            df = remove_ngrams_from_text(ngrams_df, df,source_text='ngram_removal_text')\n",
    "           \n",
    "            i -= 1  \n",
    "        except Exception as e:\n",
    "            print(str(i)+' length not found')\n",
    "            i -= 1  \n",
    "\n",
    "    return df, final_ngrams_df\n",
    "\n",
    "\n",
    "\n",
    "# remove ngrams iteratively\n",
    "def remove_text(patterns, df, source_text='final_text'):\n",
    "    # Initialize the final_text_removed column with NaN\n",
    "    df['final_text_removed'] = np.nan\n",
    "    # Compile patterns into regular expressions that match whole words, with optional punctuation at the boundaries\n",
    "    regex_patterns = [re.compile(r'\\b' + re.escape(pattern) + r'\\b[\\.\\,]?', re.IGNORECASE) for pattern in patterns]\n",
    "\n",
    "    for i in trange(len(df)):\n",
    "        text = df[source_text][i]\n",
    "        modified_text = text  # Initialize modified text\n",
    "        for pattern in regex_patterns:\n",
    "            # Replace the ngram in the modified text with the replacement string\n",
    "            modified_text = pattern.sub(lambda x: ' '.join(['*' for _ in x.group().split()]), modified_text)\n",
    "        # Update the text in the DataFrame with the modified text\n",
    "        df.loc[i, 'final_text_removed'] = modified_text\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# get the list of the remaining words after the remove_text function was applied\n",
    "def get_remaining_words(df, column='final_text_removed'):\n",
    "    # Create a set to store unique remaining words\n",
    "    remaining_words = set()\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for text in df[column]:\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "        # Update the set with words that do not contain '*' and are not punctuation, stripping punctuation from ends\n",
    "        remaining_words.update(\n",
    "            word.strip(string.punctuation) for word in words \n",
    "            if '*' not in word and not all(char in string.punctuation for char in word)\n",
    "        )\n",
    "\n",
    "    # Convert the set to a list and return\n",
    "    return list(remaining_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba957c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
